{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import nni\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nni.utils import merge_parameter\n",
    "from torchvision import datasets, transforms\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as torch_optim\n",
    "from torchvision import models\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger('AutoDL')\n",
    "\n",
    "def bn_drop_lin(n_in: int, n_out: int, bn: bool = True, p: float = 0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers\n",
    "\n",
    "\n",
    "def ifnone(a, b):\n",
    "    \"`a` if `a` is not None, otherwise `b`.\"\n",
    "    return b if a is None else a\n",
    "\n",
    "\n",
    "def listify(p, q):\n",
    "    \"Make `p` listy and the same length as `q`.\"\n",
    "    if p is None:\n",
    "        p = []\n",
    "    elif isinstance(p, str):\n",
    "        p = [p]\n",
    "    elif not isinstance(p, Iterable):\n",
    "        p = [p]\n",
    "    # Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    else:\n",
    "        try:\n",
    "            a = len(p)\n",
    "        except:\n",
    "            p = [p]\n",
    "    n = q if type(q) == int else len(p) if q is None else len(q)\n",
    "    if len(p) == 1: p = p * n\n",
    "    assert len(p) == n, f'List len mismatch ({len(p)} vs {n})'\n",
    "    return list(p)\n",
    "\n",
    "\n",
    "class TabularModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_szs, n_cont, out_sz, layers, device, ps=None,\n",
    "                 emb_drop=0., y_range=None, use_bn: bool = True, bn_final: bool = False):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0] * len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.embeds = nn.ModuleList(\n",
    "            [nn.Embedding(ni, nf) for ni, nf in emb_szs])  # type: torch.nn.modules.container.ModuleList\n",
    "        self.emb_drop = nn.Dropout(emb_drop)  # type: torch.nn.modules.dropout.Dropout\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)  # type torch.nn.modules.batchnorm.BatchNorm1d\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)  # n_emb = 17 , type: int\n",
    "        self.n_emb, self.n_cont, self.y_range = n_emb, n_cont, y_range\n",
    "        sizes = [n_emb + n_cont] + layers + [out_sz]  # typeL list, len: 4\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes) - 2)] + [\n",
    "            None]  # type: list, len: 3.  the last in None because we finish with linear\n",
    "        layers = []\n",
    "        for i, (n_in, n_out, dp, act) in enumerate(zip(sizes[:-1], sizes[1:], [0.] + ps, actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i != 0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)  # type: torch.nn.modules.container.Sequential\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "\n",
    "        if self.n_emb != 0 and x_cat is not None:\n",
    "            x_cat = x_cat.to(self.device)\n",
    "            x = [e(x_cat[:, i]) for i, e in enumerate(\n",
    "                self.embeds)]  # take the embedding list and grab an embedding and pass in our single row of data.\n",
    "            x = torch.cat(x, 1)  # concatenate it on dim 1 ## remeber that the len is the batch size\n",
    "            x = self.emb_drop(x)  # pass it through a dropout layer\n",
    "\n",
    "        if self.n_cont != 0 and x_cont is not None:\n",
    "            x_cont = x_cont.to(self.device)\n",
    "            x_cont = self.bn_cont(x_cont)  # batchnorm1d\n",
    "            x = torch.cat([x, x_cont],\n",
    "                          1) if self.n_emb != 0 else x_cont  # combine the categircal and continous variables on dim 1\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1] - self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]  # deal with y_range\n",
    "        x = x.squeeze()\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColumnarDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, cats, y):\n",
    "        self.dfcats = df[cats]  # type: pandas.core.frame.DataFrame\n",
    "        self.dfconts = df.drop(cats, axis=1)  # type: pandas.core.frame.DataFrame\n",
    "\n",
    "        if self.dfcats.shape[1] > 0:\n",
    "            self.cats = np.stack([c.values for n, c in self.dfcats.items()], axis=1).astype(\n",
    "                np.int64)  # tpye: numpy.ndarray\n",
    "        if self.dfconts.shape[1] > 0:\n",
    "            self.conts = np.stack([c.values for n, c in self.dfconts.items()], axis=1).astype(\n",
    "                np.float32)  # tpye: numpy.ndarray\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.dfcats.shape[1] > 0 and self.dfconts.shape[1] > 0:\n",
    "            return [self.cats[idx], self.conts[idx], self.y[idx]]\n",
    "        if self.dfcats.shape[1] <= 0 and self.dfconts.shape[1] > 0:\n",
    "            return [0, self.conts[idx], self.y[idx]]\n",
    "        if self.dfcats.shape[1] > 0 and self.dfconts.shape[1] <= 0:\n",
    "            return [self.cats[idx], 0., self.y[idx]]\n",
    "        return [0, 0., self.y[idx]]\n",
    "\n",
    "\n",
    "class BCE_loss_weighted(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCE_loss_weighted, self).__init__()\n",
    "\n",
    "    def forward(self, pred, y):\n",
    "        # pred is the model predicted probabilities\n",
    "        # y is labels\n",
    "\n",
    "        n_class1 = (y == 1).sum().to(dtype=torch.float)\n",
    "        n_class0 = (y == 0).sum().to(dtype=torch.float)\n",
    "\n",
    "        pred = pred.to(dtype=torch.float).squeeze()\n",
    "        y = y.to(dtype=torch.float)\n",
    "\n",
    "        weight = torch.tensor([max(n_class1, n_class0) / n_class0, max(n_class1, n_class0) / n_class1])\n",
    "        weight_ = weight[y.data.view(-1).long()].view_as(y)\n",
    "\n",
    "        loss_fn = nn.BCELoss(reduce=False)\n",
    "        loss = (loss_fn(pred, y) * weight_).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def weighted_binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = np.array([y >= 0.5 for y in y_pred]).astype(float)\n",
    "    y_test = np.array(y_test)\n",
    "    TP = ((y_pred_tag == y_test) * (y_test == 1)).sum()\n",
    "    TN = ((y_pred_tag == y_test) * (y_test == 0)).sum()\n",
    "    N = (y_test == 0).sum()\n",
    "    P = (y_test == 1).sum()\n",
    "\n",
    "    return 0.5 * (TP / P + TN / N)\n",
    "\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = np.array([y >= 0.5 for y in y_pred]).astype(float)\n",
    "    y_test = np.array(y_test)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum()\n",
    "    acc = correct_results_sum / len(y_test)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (cat, cont, target) in enumerate(train_loader):\n",
    "        cat, cont, target = cat.to(device), cont.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(cat, cont)\n",
    "        loss = BCE_loss_weighted()(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            logger.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(cont), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, metric):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    y_true_val = list()\n",
    "    y_pred_val = list()\n",
    "    with torch.no_grad():\n",
    "        for cat, cont, target in test_loader:\n",
    "            cat, cont, target = cat.to(device), cont.to(device), target.to(device)\n",
    "            output = model(cat, cont)\n",
    "            # sum up batch loss\n",
    "            test_loss += BCE_loss_weighted()(output, target)\n",
    "            # get the index of the max log-probability\n",
    "            y_true_val += list(target.cpu().data.numpy())\n",
    "            y_pred_val += list(output.cpu().data.numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    accuracy = 100. * metric(y_pred_val, y_true_val)\n",
    "\n",
    "    logger.info('\\nTest set: Average loss: {:.4f}, Accuracy: ({:.0f}%)\\n'.format(\n",
    "        test_loss, accuracy))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def main(args, path_train, target, X_train, X_val, y_train, y_val, emb_szs, num_attribs, cat_attribs):\n",
    "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
    "    torch.manual_seed(args['seed'])\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train, columns=num_attribs + cat_attribs)\n",
    "    X_val_df = pd.DataFrame(X_val, columns=num_attribs + cat_attribs)\n",
    "\n",
    "    trainds = ColumnarDataset(X_train_df, cat_attribs, np.array(y_train))\n",
    "    valds = ColumnarDataset(X_val_df, cat_attribs, np.array(y_val))\n",
    "\n",
    "    params = {'batch_size': 2048,\n",
    "              'shuffle': True}\n",
    "\n",
    "    traindl = torch.utils.data.DataLoader(trainds, **params)\n",
    "    valdl = torch.utils.data.DataLoader(valds, **params)\n",
    "\n",
    "    hidden_size = args['hidden_size']\n",
    "    model = TabularModel(emb_szs=emb_szs, n_cont=len(num_attribs), out_sz=1, layers=[hidden_size, hidden_size // 2],\n",
    "                         ps=[args['ps1'], args['ps2']], emb_drop=args['emb_drop'], device=device).to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
    "\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        train(args, model, device, traindl, optimizer, epoch)\n",
    "        test_acc = test(args, model, device, valdl, weighted_binary_acc)\n",
    "        print(test_acc)\n",
    "\n",
    "    # report final result\n",
    "    logger.debug('Final result is %g', test_acc)\n",
    "    return test_acc\n",
    "\n",
    "def get_params():\n",
    "    # Training settings\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='AutoDL')\n",
    "    parser.add_argument('--batch_size', type=int, default=2048, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument(\"--hidden_size\", type=int, default=1000, metavar='N',\n",
    "                        help='hidden layer size (default: 1000)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 1)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--no_cuda', action='store_true', default=True,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--ps1', type=float, default=0.1, help='Dropout for first layer')\n",
    "    parser.add_argument('--ps2', type=float, default=0.05, help='Dropout for second layer')\n",
    "    parser.add_argument('--emb_drop', type=float, default=0.04, help='Dropout for categorical embeddings')\n",
    "\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark pour un seull dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_deep(dataset_name,target_name):\n",
    "    # Reading dataset\n",
    "    path_train = os.path.join(dir,dataset_name+'_train.csv')\n",
    "    df = pd.read_csv(path_train)\n",
    "    \n",
    "    X = df.drop(target_name,1)\n",
    "    y = df[target_name]\n",
    "    \n",
    "    num_attribs = list(X.select_dtypes(include=[np.number]))\n",
    "    cat_attribs = list(X.columns.drop(num_attribs))\n",
    "\n",
    "    #Cleaning and preprocessing pipelines\n",
    "    #Cleaning and preprocessing pipelines\n",
    "    num_pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy=\"median\")), # replace Null by median\n",
    "            ('std_scaler', StandardScaler()), # mean 0 std 1\n",
    "        ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "            ('cat_imputer', SimpleImputer(strategy='most_frequent')), \n",
    "            ('encoder', OrdinalEncoder()), \n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attribs),\n",
    "            (\"cat\", cat_pipeline, cat_attribs),\n",
    "        ])\n",
    "\n",
    "    #Training Data prepared \n",
    "    X_prepared = full_pipeline.fit_transform(X)\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    y_prepared = [x[0] for x in ordinal_encoder.fit_transform(pd.DataFrame(y))]\n",
    "\n",
    "    #Splitting to train and test\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_prepared, y_prepared, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for col in cat_attribs:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    cat_szs = [len(df[col].cat.categories) for col in cat_attribs]\n",
    "    emb_szs = [(size, min(50, (size + 1) // 2)) for size in cat_szs]\n",
    "    \n",
    "    benchmark_dict = {}\n",
    "    ########### default RF\n",
    "\n",
    "    RF_clf = RandomForestClassifier()\n",
    "    t0_RF = time.time()\n",
    "    RF_clf.fit(X_train, y_train)\n",
    "    RF_acc = RF_clf.score(X_val, y_val)\n",
    "    RF_acc_weighted = weighted_binary_acc(RF_clf.predict(X_val), y_val)\n",
    "    t_RF = time.time() - t0_RF\n",
    "    benchmark_dict['RF'] = {}\n",
    "    benchmark_dict['RF']['acc'] = RF_acc\n",
    "    benchmark_dict['RF']['acc_weighted'] = RF_acc_weighted\n",
    "    benchmark_dict['RF']['time'] = t_RF\n",
    "    \n",
    "    ########### default XGBoost\n",
    "    t0_XGB = time.time()\n",
    "    XGB_clf = XGBClassifier()\n",
    "    XGB_clf.fit(X_train, y_train)\n",
    "    XGB_acc = XGB_clf.score(X_val, y_val)\n",
    "    XGB_acc_weighted = weighted_binary_acc(XGB_clf.predict(X_val), y_val)\n",
    "    t_XGB = time.time() - t0_XGB\n",
    "    benchmark_dict['XGB'] = {}\n",
    "    benchmark_dict['XGB']['acc'] = XGB_acc\n",
    "    benchmark_dict['XGB']['acc_weighted'] = XGB_acc_weighted\n",
    "    benchmark_dict['XGB']['time'] = t_XGB\n",
    "\n",
    "    ########### default LightGBM\n",
    "    t0_LGBM = time.time()\n",
    "    LGBM_clf = LGBMClassifier()\n",
    "    LGBM_clf.fit(X_train, y_train)\n",
    "    LGBM_acc = LGBM_clf.score(X_val, y_val)\n",
    "    LGBM_acc_weighted = weighted_binary_acc(LGBM_clf.predict(X_val), y_val)\n",
    "    t_LGBM = time.time() - t0_LGBM\n",
    "    benchmark_dict['LGBM'] = {}\n",
    "    benchmark_dict['LGBM']['acc'] = LGBM_acc\n",
    "    benchmark_dict['LGBM']['acc_weighted'] = LGBM_acc_weighted\n",
    "    benchmark_dict['LGBM']['time'] = t_LGBM\n",
    "    \n",
    "    ######### deep learning MLP Skortch##########\n",
    "    args = vars(get_params())\n",
    "    t_deep_0 = time.time()\n",
    "    MLP_acc_weighted = main(args, path_train, target, X_train, X_val, y_train, y_val, emb_szs, num_attribs, cat_attribs)\n",
    "    t_MLP = time.time() - t_deep_0\n",
    "    benchmark_dict['MLP'] = {}\n",
    "    benchmark_dict['MLP']['acc_weighted'] = MLP_acc_weighted\n",
    "    benchmark_dict['MLP']['time'] = t_MLP\n",
    "    \n",
    "    \n",
    "    return benchmark_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove DADGP and TARGET_FALL_TRESO before running, DADGP is not available to the public !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['banking','bnp_cardif_claims','home-credit-default-risk','DADGP']\n",
    "targets = ['subscribed','target','TARGET','TARGET_FALL_TRESO']\n",
    "dir = 'Deep_benchmark_datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boucle sur les 4 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\azizeac\\casa-distributed-mlbox\\venv\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "c:\\users\\azizeac\\casa-distributed-mlbox\\venv\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.69442325017663\n",
      "DADGP is done\n"
     ]
    }
   ],
   "source": [
    "benchmark_total ={}\n",
    "for i in range(len(datasets)):\n",
    "    dataset_name = datasets[i]\n",
    "    target = targets[i]\n",
    "    benchmark_total[dataset_name] = benchmark_deep(dataset_name,target)\n",
    "    print(f'{dataset_name} is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banking': {'RF': {'acc': 0.9031595576619273, 'acc_weighted': 0.6990552348487088, 'time': 0.38228273391723633}, 'XGB': {'acc': 0.9116903633491311, 'acc_weighted': 0.7389644036982371, 'time': 2.84494686126709}, 'LGBM': {'acc': 0.9131121642969984, 'acc_weighted': 0.7576089046583216, 'time': 0.7343833446502686}, 'MLP': {'acc_weighted': 84.00693987518291, 'time': 6.159183025360107}}, 'bnp_cardif_claims': {'RF': {'acc': 0.7521976820467964, 'acc_weighted': 0.6068927543476134, 'time': 10.002477169036865}, 'XGB': {'acc': 0.7833807128799475, 'acc_weighted': 0.5809906190454418, 'time': 98.27176594734192}, 'LGBM': {'acc': 0.7846052919308988, 'acc_weighted': 0.5790185259586512, 'time': 13.405271053314209}, 'MLP': {'acc_weighted': 65.96188718043156, 'time': 28.193854808807373}}, 'home-credit-default-risk': {'RF': {'acc': 0.9184104840414289, 'acc_weighted': 0.5060276006338672, 'time': 17.698732376098633}, 'XGB': {'acc': 0.9196787148594378, 'acc_weighted': 0.5028452520765384, 'time': 144.76394867897034}, 'LGBM': {'acc': 0.9195974180121295, 'acc_weighted': 0.5076870849292912, 'time': 25.892030715942383}, 'MLP': {'acc_weighted': 66.96543077974395, 'time': 70.7586977481842}}}\n"
     ]
    }
   ],
   "source": [
    "print(benchmark_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DADGP': {'RF': {'acc': 0.8633999786490979, 'acc_weighted': 0.8639218308806845, 'time': 76.59263730049133}, 'XGB': {'acc': 0.8463040063442681, 'acc_weighted': 0.8450095643817688, 'time': 1277.9020190238953}, 'LGBM': {'acc': 0.8546613594424365, 'acc_weighted': 0.8530927880918535, 'time': 176.50830507278442}, 'MLP': {'acc_weighted': 83.69442325017663, 'time': 69.5585150718689}}}\n"
     ]
    }
   ],
   "source": [
    "print(benchmark_total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
